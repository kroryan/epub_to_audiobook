#!/usr/bin/env python3
"""
Test the updated chunking system for XTTS token limits
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from audiobook_generator.tts_providers.coqui_tts_provider import CoquiTTSProvider

def test_token_based_chunking():
    """Test that the new token-based chunking works correctly"""
    
    # Create a mock config
    class MockConfig:
        def __init__(self):
            self.coqui_model = "tts_models/multilingual/multi-dataset/xtts_v2"
            self.coqui_language = "es"
            self.coqui_speaker = "Viktor Eka"
            self.output_format = "mp3"
    
    config = MockConfig()
    provider = CoquiTTSProvider(config)
    
    # Test text that would cause the 400 token error
    long_text = """
    En la Torre Flotante, Elminster se encontrÃ³ con una gran aventura. El terror ciego se apoderÃ³ de Ã©l
    cuando vio las criaturas mÃ¡gicas que habitaban en los pasillos oscuros. Cada paso que daba resonaba
    en el silencio mortal que envolvÃ­a el lugar. Las paredes estaban cubiertas de runas antiguas que 
    brillaban con una luz azulada, creando sombras danzantes que parecÃ­an cobrar vida propia. El aire
    estaba cargado de magia elemental, tan denso que casi se podÃ­a cortar con una espada. Los hechizos
    de protecciÃ³n que habÃ­a lanzado antes de entrar comenzaban a debilitarse, y podÃ­a sentir cÃ³mo las
    fuerzas oscuras intentaban penetrar sus defensas mÃ¡gicas. A medida que avanzaba hacia el centro
    de la torre, los sonidos se volvÃ­an mÃ¡s extraÃ±os y amenazadores. Susurros en idiomas olvidados
    llenaban el aire, y ocasionalmente podÃ­a escuchar el eco de pasos que no eran los suyos.
    La aventura que habÃ­a comenzado como una simple misiÃ³n de reconocimiento se habÃ­a convertido
    en una lucha por la supervivencia en un lugar donde la realidad misma parecÃ­a estar distorsionada
    por la antigua magia que impregnaba cada piedra de la construcciÃ³n flotante.
    """ * 3  # Hacer el texto aÃºn mÃ¡s largo
    
    print(f"ğŸ§ª Testing text of {len(long_text)} characters...")
    
    # Test chunking
    chunks = provider._split_text_for_xtts(long_text, max_tokens=350)
    
    print(f"âœ… Successfully split into {len(chunks)} chunks")
    
    # Verify all chunks are within limits
    max_chunk_size = 0
    for i, chunk in enumerate(chunks):
        chunk_size = len(chunk)
        word_count = len(chunk.split())
        estimated_tokens = word_count * 1.3  # Rough estimation
        
        print(f"  Chunk {i+1}: {chunk_size} chars, ~{word_count} words, ~{estimated_tokens:.0f} tokens")
        
        if chunk_size > max_chunk_size:
            max_chunk_size = chunk_size
        
        # Check if chunk is within safe limits (should be ~1050 chars max)
        if chunk_size > 1200:
            print(f"âš ï¸  Warning: Chunk {i+1} might be too large ({chunk_size} chars)")
        
        # Preview chunk content
        preview = chunk[:100].replace('\n', ' ').strip()
        print(f"    Preview: {preview}...")
    
    print(f"\nğŸ“Š Max chunk size: {max_chunk_size} characters")
    
    # Test text reconstruction
    reconstructed = "\n\n".join(chunks)
    original_words = len(long_text.split())
    reconstructed_words = len(reconstructed.split())
    
    print(f"ğŸ“ Original text: {original_words} words")
    print(f"ğŸ“ Reconstructed: {reconstructed_words} words")
    
    if abs(original_words - reconstructed_words) <= 5:  # Allow small differences due to spacing
        print("âœ… Text integrity maintained!")
    else:
        print("âŒ Text integrity compromised!")
    
    return chunks

if __name__ == "__main__":
    test_token_based_chunking()